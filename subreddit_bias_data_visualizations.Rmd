---
title: "Subreddit Bias Data Visualizations and Summaries"
author: "Drew Walker"
date: "4/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(tidytext)
library(lubridate)
library(stringdist)
```

```{r ggplot_word}

all_posts_df_body <- read_csv("all_subreddit_bias_posts.csv")
all_posts_df_tokenized <- all_posts_df_body %>% 
  unnest_tokens(sentence,body, token="sentences")

all_posts_df <- left_join(all_posts_df_tokenized,all_posts_df_body, by = "id_") 

word_list_with_stem <- read_csv("word_list_round_2.csv")
word_list_with_stem$similar_word <- str_replace_all(word_list_with_stem$similar_word,"_"," ")
#bias_words <- as.tibble(c("abuser","junkie","alcoholic","drunk","habit","dirty","stigma","bias","stereotype","shame","blame"))
#"abuser|junkie|alcoholic|drunk|habit|dirty|stigma|bias|stereotype|shame|blame"

bias_words_df <- read_csv("expanded_misspellings.csv")
#Columns 0-27 include all misspelling words
no_misspellings_word_list <- bias_words_df %>% 
  pivot_longer(!X1, names_to = "number",values_to ="ms_word") %>% 
  filter(number == "0") %>% 
  filter(is.na(ms_word)) %>% 
  select(regex = X1) %>% 
  mutate(similar_word = regex) %>% 
  left_join(word_list_with_stem,by = "similar_word")

bias_words_list <- bias_words_df %>% 
  pivot_longer(!X1, names_to = "number",values_to ="ms_word") %>% 
  drop_na() %>% 
  rename(similar_word = X1) %>% 
  left_join(word_list_with_stem, by = "similar_word") %>% 
  group_by(stem_word) %>% 
  mutate(regex = paste(ms_word,collapse="|")) %>% 
  distinct(stem_word,.keep_all = TRUE) %>% 
  select(stem_word,regex,similar_word)

regex_list <- bind_rows(no_misspellings_word_list,bias_words_list) 

regex_list2a <- regex_list %>% 
    group_by(stem_word) %>% 
  mutate(regex2 = paste(regex,collapse="|")) %>% 
  distinct(stem_word,.keep_all = TRUE)  

#Regex 3 will be experiment to see if we can remove the words from regex that are not of max cosign similarity (to get rid of overlapping addict terms)

regex_list2 <- regex_list2a %>% 
  mutate(regexlist = as.list(strsplit(regex2, "|", fixed=TRUE))) %>% 
  unnest(regexlist) %>% 
  mutate(stem_similarity = stringdist(regexlist,stem_word,method = "cosine")) %>% 
  group_by(regexlist) %>% 
  filter(stem_similarity == min(stem_similarity)) %>% 
  ungroup() %>% 
  group_by(stem_word) %>% 
    mutate(regex2 = paste(regexlist,collapse="|")) %>% 
  distinct(stem_word, .keep_all= TRUE)


all_posts_counts <- all_posts_df_body %>% 
    mutate(day = as.Date(date(created)),
         month_year = floor_date(day, "month")) %>% group_by(month_year) %>% 
  mutate(all_sub_count = n(), 
           total_popularity = sum(score)) %>% 
  distinct(month_year,all_sub_count,total_popularity)

# Function that builds summary data frame and ggplot (split into two). word = regex for stem word + expanded words + misspellings 
get_summary_plot <- function(word){
word_df <- all_posts_df_body %>%
  filter(str_detect(body, word)) %>% 
  mutate(day = as.Date(date(created)),
         month_year = floor_date(day, "month")) %>% 
  group_by(month_year) %>% 
  mutate(count = n(),
         word_popularity = sum(score),
         regex2 = word) %>% 
  left_join(regex_list2, by = "regex2") %>%  left_join(all_posts_counts, by = "month_year") %>% mutate(normalized_count = count/all_sub_count,
         norm_hundred = normalized_count*100,
         normalized_popularity = word_popularity/total_popularity,
         norm_pop_hundred = normalized_popularity*100)
  
plot_obs <- ggplot(data=word_df,         
                   aes(x = month_year,
                       y = norm_hundred))+
  geom_line()+
  #adding lines to plot
                   theme_classic() + 
  ggtitle(paste(word_df$stem_word))+
                   scale_y_continuous(name = "Comments/posts per 100") 
return(plot_obs)
}

get_summary_df <- function(word){
word_df <- all_posts_df_body %>%
  filter(str_detect(body, word)) %>% 
  mutate(day = as.Date(date(created)),
         month_year = floor_date(day, "month")) %>% 
  group_by(month_year) %>% 
  mutate(count = n(),
         regex2 = word) %>% 
  left_join(regex_list2, by = "regex2") %>%  left_join(all_posts_counts, by = "month_year") %>% mutate(normalized_count = count/all_sub_count,
         norm_hundred = normalized_count*100)
return(word_df)}
test_df <- get_summary_df("users|user")
test_plot <- get_summary_plot("users|user")
test_plot

```


# Popularity plots
(Number of likes posts with each word receives, divided by total likes on posts for each month)

```{r popularity-plots}
get_popularity_plot <- function(word){
word_df <- all_posts_df_body %>%
  filter(str_detect(body, word)) %>% 
  mutate(day = as.Date(date(created)),
         month_year = floor_date(day, "month")) %>% 
  group_by(month_year) %>% 
  mutate(count = n(),
         word_popularity = sum(score),
         regex2 = word) %>% 
  left_join(regex_list2, by = "regex2") %>%  left_join(all_posts_counts, by = "month_year") %>% mutate(normalized_count = count/all_sub_count,
         norm_hundred = normalized_count*100,
         normalized_popularity = word_popularity/total_popularity,
         norm_pop_hundred = normalized_popularity*100)
  
plot_obs <- ggplot(data=word_df,         
                   aes(x = month_year,
                       y = norm_pop_hundred))+
  geom_line()+
  #adding lines to plot
                   theme_classic() + 
  ggtitle(paste(word_df$stem_word))+
                   scale_y_continuous(name = "Monthly Likes per 100 Likes in Subreddits") 
return(plot_obs)
}
```



```{r top-words}
# Need to find a way to bring in stem word, join with word count df 
library(tidytext)
library(ggpubr)
stem_word_and_regex <- regex_list2 %>% 
  select(stem_word, regex=regex2)

get_trigrams <-
 function(word){
word_df <- all_posts_df_body %>%
  filter(str_detect(body, word))
stem_word = as_tibble(word) %>% 
  select(regex = value)
regex_for_word <- left_join(stem_word,stem_word_and_regex)

word_df$body <- gsub('[[:punct:]]+', '', word_df$body)
text_cleaning_tokens <- word_df %>% 
  tidytext::unnest_tokens(ngram, body,token = "ngrams",n=3) 
#remove words? like 
#text_cleaning_tokens$ngram <- gsub('[[:digit:]]+', '', text_cleaning_tokens$ngram)
text_cleaning_tokens$ngram <- gsub('^https|amp','', text_cleaning_tokens$ngram)

text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(ngram) == 1))%>% 
  separate(ngram, into = c("first","second","third"), sep = " ", remove = FALSE) %>%
  anti_join(stop_words, by = c("first" = "word")) %>%
  anti_join(stop_words, by = c("second" = "word")) %>% 
  anti_join(stop_words, by = c("third" = "word"))

word_count <- text_cleaning_tokens %>% 
  filter(ngram != "") %>% 
  count(ngram) %>% 
  arrange(-n) %>% 
  slice_max(n, n = 5) %>% 
  mutate(stem_word = regex_for_word$stem_word)
word_count_bar <- ggplot(word_count, aes(x= fct_reorder(ngram,n),y = n, fill = ngram)) + 
          geom_col(show.legend = FALSE)+
  labs(x = "Trigram", y = "Count in Subreddit Posts")+
  ggtitle(paste("Top Trigrams of Posts matching:", word_count$stem_word))+
  coord_flip()+
  theme_pubclean()
 return(word_count_bar)
}

get_bigrams <-
 function(word){
word_df <- all_posts_df_body %>%
  filter(str_detect(body, word))
stem_word = as_tibble(word) %>% 
  select(regex = value)
regex_for_word <- left_join(stem_word,stem_word_and_regex)

word_df$body <- gsub('[[:punct:]]+', '', word_df$body)
text_cleaning_tokens <- word_df %>% 
  tidytext::unnest_tokens(ngram, body,token = "ngrams",n=2) 
#remove words? like 
#text_cleaning_tokens$ngram <- gsub('[[:digit:]]+', '', text_cleaning_tokens$ngram)
text_cleaning_tokens$ngram <- gsub('^https|amp','', text_cleaning_tokens$ngram)

text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(ngram) == 1))%>% 
  separate(ngram, into = c("first","second"), sep = " ", remove = FALSE) %>%
  anti_join(stop_words, by = c("first" = "word")) %>%
  anti_join(stop_words, by = c("second" = "word"))
## count ngrams
word_count <- text_cleaning_tokens %>% 
  filter(ngram != "") %>% 
  count(ngram) %>% 
  arrange(-n) %>% 
  slice_max(n, n = 15) %>% 
  mutate(stem_word = regex_for_word$stem_word)
word_count_bar <- ggplot(word_count, aes(x= fct_reorder(ngram,n),y = n, fill = ngram)) + 
          geom_col(show.legend = FALSE)+
  labs(x = "Bigram", y = "Count in Subreddit Posts")+
  ggtitle(paste("Top Bigrams of Posts matching:", word_count$stem_word))+
  coord_flip()+
  theme_pubclean()
 return(word_count_bar)
}
# Get top words
get_top_words <-
 function(word){
word_df <- all_posts_df_body %>%
  filter(str_detect(body, word))
stem_word = as_tibble(word) %>% 
  select(regex = value)
regex_for_word <- left_join(stem_word,stem_word_and_regex)

word_df$body <- gsub('[[:punct:]]+', '', word_df$body)
text_cleaning_tokens <- word_df %>% 
  tidytext::unnest_tokens(word, body) 
#remove words? like 
#text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('^https|amp','', text_cleaning_tokens$word)
text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)

## count ngrams

word_count <- text_cleaning_tokens %>% 
  filter(word != "") %>% 
  count(word) %>% 
  arrange(-n) %>% 
  slice_max(n, n = 15) %>% 
  mutate(stem_word = regex_for_word$stem_word)
word_count_bar <- ggplot(word_count, aes(x= fct_reorder(word,n),y = n, fill = word)) + 
          geom_col(show.legend = FALSE)+
  labs(x = "Word", y = "Count in Subreddit Posts")+
  ggtitle(paste("Posts matching:", word_count$stem_word))+
  coord_flip()+
  theme_pubclean()
 return(word_count_bar)
}

```


iteration
```{r iterated-bias-sample}
library(purrr)
library(furrr)
library(gridExtra)
library(patchwork)
# make functions set to catch errors with NA
possible_summary_plot <- purrr::possibly(get_summary_plot, otherwise = tidyr::tibble("NA"))
possible_summary_df <- purrr::possibly(get_summary_df, otherwise = tidyr::tibble("NA"))
possible_get_trigrams <- purrr::possibly(get_trigrams, otherwise = tidyr::tibble("NA"))
possible_get_bigrams <- purrr::possibly(get_bigrams, otherwise = tidyr::tibble("NA"))
possible_get_top_words <- purrr::possibly(get_top_words, otherwise = tidyr::tibble("NA"))
possible_get_popularity_plot <- purrr::possibly(get_popularity_plot, otherwise = tidyr::tibble("NA"))

bias_words_full <- regex_list2 %>% 
  mutate(summary_plot = future_map(regex2, possible_summary_plot),
         summary_df = future_map(regex2,possible_summary_df),
         trigrams = future_map(regex2,possible_get_trigrams),
         bigrams = future_map(regex2, possible_get_bigrams),
         top_words = future_map(regex2,possible_get_top_words),
         popularity_plot = future_map(regex2,possible_get_popularity_plot))
# Normalized monthly terms per 100 posts 
plots <- bias_words_full$summary_plot
wrap_plots(plots)
ggsave('bias_terms_plots.png', width = 15, height = 10)

#popularity plots 
pop_plots <- bias_words_full$popularity_plot
wrap_plots(pop_plots)
ggsave('bias_terms_popularity_plots.png', width = 15, height = 10)

# Top words plots
top_words <- bias_words_full$top_words
wrap_plots(top_words)
ggsave('top_words_plots.png', width = 25, height = 20)

# Top bigrams plots
bigrams <- bias_words_full$bigrams
wrap_plots(bigrams)
ggsave('top_bigrams_plots.png', width = 25, height = 20)


# Top trigrams plots
trigrams <- bias_words_full$trigrams
wrap_plots(trigrams)
ggsave('top_trigrams_plots.png', width = 25, height = 20, limitsize = FALSE)

write_csv(stem_word_and_regex, "stem_word_and_regex.csv")
```

# Total posts plot

Showing total number of comments and posts by month
```{r totals-plot}
total_word_df <- all_posts_df_body %>%
  mutate(day = as.Date(date(created)),
         month_year = floor_date(day, "month")) %>% 
  group_by(month_year) %>% 
  mutate(count = n(),
         total_popularity = sum(score)) 
total_plot_obs <- ggplot(data=total_word_df,         
                   aes(x = month_year,
                       y = count))+
  geom_line()+
  #adding lines to plot
                   theme_classic() + 
  ggtitle("Total Monthly Posts and Comments in MAT-Related Subreddits")+
                   scale_y_continuous(name = "Total Monthly posts and comments") 

total_plot_obs
ggsave("total_monthly_posts_and_comments.png")

#Users, 3308 in sample. # out before publication
#all_posts_df %>% 
#  group_by(author) %>% 
#  distinct(author)
# Can i get % of users that have used that term in a post? 
```
plot and df of all bias terms
```{r plot-summary-bias-terms}
get_all_words_plot <- function(word){
word_df <- all_posts_df_body %>%
  filter(str_detect(body, word)) %>% 
  mutate(day = as.Date(date(created)),
         month_year = floor_date(day, "month")) %>% 
  group_by(month_year) %>% 
  mutate(count = n(),
         word_popularity = sum(score),
         regex2 = word) %>% 
  left_join(regex_list2, by = "regex2") %>%  left_join(all_posts_counts, by = "month_year") %>% mutate(normalized_count = count/all_sub_count,
         norm_hundred = normalized_count*100,
         normalized_popularity = word_popularity/total_popularity,
         norm_pop_hundred = normalized_popularity*100)
  
plot_obs <- ggplot(data=word_df,         
                   aes(x = month_year,
                       y = norm_hundred))+
  geom_line()+
  #adding lines to plot
                   theme_classic() + 
  ggtitle("Percentage of Posts with Words in Stigma Lexicon")+
                   scale_y_continuous(name = "Comments/posts per 100") 
return(plot_obs)
}



regex_list2$regex2
all_bias_words_regex <- paste0(regex_list2$regex2, sep = "|", collapse = "")

all_bias_words_regex <- str_sub(all_bias_words_regex,1,nchar(all_bias_words_regex)-1)


get_all_words_plot(all_bias_words_regex)

ggsave("all_stigma_words_regex_plot.png")


all_bias_words_regex_df <- get_summary_df(all_bias_words_regex)

# Looks like a majority of posts contain these terms. 
# Next steps: Word by word validation, see if any are poorly performing in capturing experiences of bias. Remove those, and then run the combined regex of all the most relevant terms, and then test that combined regex qualitatively 
```

# Create sample of 10 posts including a word within each of the 12 stemword-related expanded lexicon 

```{r sampling-for-coding}


random_posts_per_stem <- function(stem_word){
set.seed(424242)
stem_word_df = tibble(stem_word)
word_and_regex <- left_join(stem_word_df,stem_word_and_regex, by = "stem_word")
random_df <- all_posts_df %>% 
  filter(str_detect(sentence,word_and_regex$regex)) %>%  #Change body to sentence 
  slice_sample(n=10) %>% 
  mutate(bias_and_stigma_described = "",
         coder = "",
         stem_word = stem_word,
         regex = word_and_regex$regex)
return(random_df)
}

random_posts_per_stem("junkie")
library(purrr)
library(furrr)
possible_none <- purrr::possibly(random_posts_per_stem, otherwise = tidyr::tibble("NA"))
words_dfs <- stem_word_and_regex$stem_word %>% 
  map_dfr(random_posts_per_stem)#place the sentence and body close together to be able to see where it is in the post

words_dfs_clean <- words_dfs %>% 
  select(id = id_,
         body,
         sentence,
         bias_and_stigma_described,
         coder,
         stem_word,
         regex)
write_csv(words_dfs_clean,"random_moud_subreddit_posts_bias_lexicon.csv")
```

